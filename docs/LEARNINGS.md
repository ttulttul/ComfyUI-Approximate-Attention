# Learnings

- ComfyUI's attention functions are wrapped with `optimized_attention_override` via `transformer_options`, which lets a custom node swap attention backends without patching core code.
- Flux attention calls `optimized_attention(..., skip_reshape=True)` with `q/k/v` shaped `[B, H, N, D]` after RoPE, so backend overrides should handle that layout.
- Taylor attention now estimates its own activation memory and calls ComfyUI's `model_management.free_memory` to prompt offloading before large allocations.
- Early-probe and fp32 denominator options help avoid slow Taylor fallbacks when denominators go unstable.
- Sub-head block Taylor splits each head into smaller blocks to reduce feature dimension while keeping P fixed.
- Defaults now target diffusion-scale workloads (low min_tokens, sub-head blocks, and tighter block sizes).
- Taylor now aggregates denominator/quality stats per sampling step and logs a single summary per step.
- Added qk_normalize and scale_mul knobs to stabilize P=4 Taylor attention by shrinking q·k values.
- Added qk_norm_clip and qk_norm_power to stabilize P=4 without fully normalizing Q/K.
- Denominator fallbacks can now be gated by a fraction threshold to avoid over-triggering.
- Quality stats are now computed against unmodified attention, even when Q/K are adjusted.
- Auto-tune mode can search q/k scaling during early steps and lock in a best config.
- Step logs now include q/k norm and sampled q·k percentile diagnostics to measure regime mismatch.
- Added a Triton fused-kernel path that streams Taylor feature chunks to avoid full feature tensor allocation.
- Early-probe fallbacks now respect denom_fallback_frac_limit and log probe stats for debugging.
- Step stats now surface fallback reasons and max_head_dim for easier triage.
- Added fused value-dimension chunking and bf16 S-chunk storage to reduce memory in fused path.
- Step stats now log both quality_raw (unmodified) and quality_eff (modified) metrics.
- Feature-dimension fallback logs now always include the rejected R value.
- max_feature_dim_R UI limit raised to allow very large feature dimensions when experimenting with higher P.
- For P>=5, the fused path streams multiset feature indices on GPU to avoid Python tuple construction overhead.
- Added qk_norm_sigma_max to gate q/k normalization by sigma.
- Streaming fused memory reservation now estimates based on chunk sizes to avoid over-offloading.
- Added taylor_sigma_max and taylor_layer_start/end gates to skip Taylor at high sigma or outside block ranges.
- quality_raw and quality_eff now share the same sampled query indices for apples-to-apples comparisons.
- Added a fully fused Triton path (fused_full_kernel) that uses precomputed feature tables to avoid Python feature loops.
- Added a hybrid local/global attention node that patches Flux attention to combine local RoPE attention with a global low-dim Taylor approximation.
- Hybrid attention now patches both `flux.math` and `flux.layers` attention bindings via model pre-run/cleanup callbacks.
- Model-level callbacks must be registered on `ModelPatcher` (transformer_options callbacks are not invoked by pre-run/cleanup).
- Hybrid PCA now falls back to an identity projection when too few samples are available to fit a low-rank basis.
- Hybrid attention can optionally aggregate hybrid-vs-exact quality stats across steps and log once at cleanup.
- PCA projection cache now keys by dtype to avoid bf16/float32 matmul mismatches when force_fp32 is disabled.
- Hybrid local window can be scheduled by sigma (min/max + sigma low/high) to use full attention early and windowed attention later.
- Hybrid global weight ramp now decreases with sigma (full at low sigma, zero at high sigma) so approximations can be enabled later in sampling.
- Local window scheduling treats a value of 0 as "full attention" and substitutes the current sequence length during interpolation.
- Local window scheduling now uses a smoothstep (sigmoid-like) curve for gentler start/end transitions.
- Hybrid global Taylor now aligns accumulation dtype with force_fp32 to avoid bf16/fp32 einsum mismatches.
- Hybrid quality stats logs now include the active hybrid config parameters for reproducibility.
- Hybrid quality stats now append to `output/hybrid-attention-results.jsonl` including config and inferred meta (sigma, shapes, latent resolution where possible).
- Added ClockedSweepValues node for distributing test values evenly across a clock list.
- ClockedSweepValues accepts a single integer string to generate a 1..N clock and can infer clock length from values if left blank.
- Added Combinations node to generate repeated value lists covering all combinations across inputs.
- ClockedSweepValues and Combinations outputs are marked as float list outputs so downstream nodes receive expanded list values.
- Flux2TTR patches Flux attention via pre-run/cleanup callbacks and resolves per-layer TTR modules using `transformer_options["block_type"]` and `block_index`, so single-block replacements can stay layer-specific without editing ComfyUI core.
- ComfyUI node execution may run under `torch.inference_mode()`, so any in-node distillation/training must explicitly use `torch.inference_mode(False)` and `torch.enable_grad()` around backward passes.
- Flux2TTR inference speed improved substantially by replacing per-token Python scan loops with chunked vectorized prefix updates (`cumsum` over `k⊗v`), and by running CUDA inference in input dtype (bf16/fp16) while keeping training in fp32.
- Flux2TTR quality collapsed when trained on synthetic calibration tensors; distillation must use native Flux attention outputs from real attention calls as teacher targets during sampling.
- PyTorch blocks autograd when `nn.Linear` receives inference-mode tensors; for ComfyUI online distillation we must clone q/k/v (and teacher targets) inside `torch.inference_mode(False)` before backward.
- Flux2TTR now performs Taylor-style VRAM reservation (`model_management.free_memory`) before training/inference attention calls, reserving `1.1x` of estimated working memory so ComfyUI can offload earlier node allocations.
- `model_management.free_memory` is not a persistent reservation token; repeated-run VRAM growth was caused by retained runtime references, so cleanup must release runtime GPU tensors and unregister runtime IDs.
- Since cleanup unregisters runtime IDs, cached node outputs may reference missing runtimes on later prompts; runtime recovery from config/checkpoint prevents accidental native-attention fallback in that case.
- Flux2TTR training is much more memory-sensitive than inference; capping training chunk size (64) and retrying scan chunks on OOM prevents giant `kv_assoc.cumsum` allocations from immediately exhausting VRAM.
- For practical Flux VRAM limits, online distillation must use bounded token sampling and graceful OOM fallback (disable training but keep teacher passthrough) to avoid aborting the whole sampler run.
- Adaptive scan chunking should be sticky per layer after an OOM retry; otherwise each attention call re-attempts known-failing chunk sizes and wastes time.
- Distillation progress logs every fixed update interval (e.g., every 10 updates) are useful because Flux2TTR training steps are attention-call based, not sampler-step based.
- Flux2TTR online distillation often needs hundreds of updates for usable loss; setting the node default to 512 steps is a better starting point than 32.
- Training-mode preview is useful for qualitative monitoring, but it adds another student forward pass; keep it optional so distillation-only runs can stay in teacher passthrough for speed.
- Distillation quality is better tracked with multiple per-layer metrics (NMSE, cosine similarity, norm/mean/std ratios, p95/p99 error tails, sampled attention KL/top-k overlap) and streamed to Comet each update for long-run monitoring.
- Flux2TTR quality/stability improved substantially after replacing bidirectional scan TTR with a normalized kernel-regression attention core plus a small landmark softmax residual; this preserves query-dependent normalization while keeping linear-time memory.
- For online distillation, subsampling only queries (while keeping full keys/values) is critical; replay-buffer training on `(q_sub, k_full, v_full, teacher_sub)` is markedly more learnable than subsampling q/k/v together.
- Per-layer EMA readiness gates are essential for fail-closed inference: layers should stay on native teacher attention until enough updates are seen and EMA loss is below threshold.
- In Flux2TTR, calling `model_management.free_memory` during attention can trigger downstream CPU/CUDA mismatches on some runs; making memory reservation opt-in (off by default) avoids these device errors.
- Keeping replay samples on GPU causes rapid VRAM growth in Flux2TTR because each sample includes full `k/v`; offloading replay buffers to CPU (reduced precision) is the practical default for long distillation runs.
- Per-layer step logs were noisy; aggregate training snapshots (ready layer list + q25–q75 ranges for loss/ema/cosine/nmse across layers) are more actionable during long distillation runs.
- CPU replay offload still needs a global memory budget; without eviction, full-k/v replay samples across many layers can trigger OS-level process kills even when GPU memory is stable.
- For Comet monitoring, cross-layer aggregate quantiles (min/p25/p50/p75/max) per metric are much more useful than sparse per-layer step logs for tracking overall distillation progress.
- Flux2TTR quality-control hooks are much easier to reason about when sigma/CFG are extracted centrally in runtime and passed through replay storage to layer forward, rather than re-derived in each call site.
- Randomizing swapped training layers per diffusion step (instead of training every eligible layer every call) reduces per-step compute and keeps distillation coverage broad over time.
- Keeping controller inference in a separate module with standalone checkpoint IO makes it possible to attach/detach Phase-2 routing without changing the Phase-1 TTR checkpoint format.
- Comet rate limiting is avoided by throttling Flux2TTR metric submissions to a fixed cadence (50 updates by default) while still logging final-step metrics.
- Controller training must execute student sampling as a differentiable function of controller mask/logits; passing precomputed student latents into trainer steps disconnects RMSE/cosine/LPIPS losses from controller gradients.
- Flux2TTR training/inference UX is cleaner when shared hyperparameters are emitted as a typed config object (`TTR_TRAINING_CONFIG`) and consumed by both phase nodes, while `run_attention` honors `controller_mask_override` and `controller_threshold` for controller training and quality/speed inference control.
- Some ComfyUI `comfy_api.latest` builds do not expose `io.VAE`; node schemas should use a compatibility fallback (`AnyType`) to keep extension loading across API variants.
- For VAE socket compatibility, some builds expose `io.Vae` (camel case) instead of `io.VAE`; checking both keeps node schemas portable.
- Seed-sweep workflows are easier to build with a dedicated deterministic seed-batch node (`RandomSeedBatch`) instead of overloading float sweep nodes for integer seed generation.
- Controller optimization steps must explicitly run inside `torch.inference_mode(False)` + `torch.enable_grad()` because ComfyUI may execute custom-node code under inference mode, which otherwise strips the grad graph and breaks `loss.backward()`.
- When controller modules themselves are constructed/loaded under inference mode, their parameters can become inference tensors; rebuilding a trainable copy from state_dict inside `inference_mode(False)` avoids backward failures in linear layers.
- Controller-trainer Comet logging is more reliable when the node logs explicit enable/disable state, requires an API key, flattens nested configs into scalar parameters, and prefixes per-step metrics (`flux2ttr_controller/*`) for easier dashboards.
- Persisting controller-trainer Comet logging across repeated workflow runs requires a stable `comet_experiment` key and avoiding per-run `experiment.end()` teardown; keeping keyed experiment handles alive prevents accidental run fragmentation.
- Comet experiment keys should be normalized at the node boundary (strip non-alnum, enforce 32-50 chars, pad short values with `X`) to avoid API rejections and implicit run splitting from malformed identifiers.
- Prompt-list workflows are simpler with a dedicated JSON loader node that strictly validates `array<string>` input, instead of overloading text widgets for long prompt batches.
- Controller training quality is more stable when mask sampling excludes non-ready Phase-1 TTR layers; forcing unready layers to full attention avoids training signal contamination from fallback-heavy student runs.
- Controller efficiency penalties must compare full-attention usage against `1 - target_ttr_ratio`; treating `target_ttr_ratio` as a full-attention budget silently under-penalizes high full-attention masks.
- LPIPS in controller training must ensure the LPIPS module and decoded RGB tensors are on the same device at call time; static init-time placement can drift and trigger CPU/CUDA mismatch errors.
- When some layers are forced to full attention (unready TTR), controller efficiency penalties and routing ratios must be computed over eligible layers only; overall ratios should be logged separately for throughput visibility.
- In controller REINFORCE under ComfyUI inference-mode execution, boolean masks used for autograd-tracked indexing must be `detach().clone()`d first to avoid "Inference tensors cannot be saved for backward" failures.
- Controller efficiency should be applied as reward shaping (`reward_quality - lambda_eff * eff_penalty`) and not as a direct differentiable loss term, otherwise low-variance penalty gradients can overwhelm the high-variance quality REINFORCE signal.
- Controller policies can collapse when Bernoulli logits saturate near 0/1; adding an entropy reward bonus over eligible layers (`+ lambda_entropy * H(p)`, detached into scalar reward) keeps mask sampling exploratory enough for REINFORCE to keep adapting.
- Flux2TTR landmark selection should scale with actual image token count (`landmark_fraction` with `landmark_min/max` clamps) instead of a fixed landmark count, so low resolutions avoid over-provisioning while high resolutions keep enough softmax anchors.
- Controller training is safer for long runs when `checkpoint_path` is set and checkpoints are written periodically (every 10 controller steps) instead of only at the end.
- Controller sawtooth regressions across runs are reduced by checkpointing/restoring trainer state (reward baseline/count and Adam optimizer state), not just controller weights.
- `docs/flux2ttr_v2_paper.tex` uses an inline `thebibliography`, so a reliable build script only needs multi-pass LaTeX (`latexmk` or two `pdflatex` passes) and no `bibtex` stage.
- Controller sigma embeddings stay effectively unused if Phase-2 training freezes one `controller_mask_override` for an entire denoise run; routing must be sampled per diffusion step (live controller path) to learn sigma-dependent policies.
- When controller modules are loaded/constructed under `inference_mode`, `ControllerTrainer` can rebuild a trainable copy; any training-time wrapper/runtime pointer must be rebound to `trainer.controller` or controller eval will fail with inference-tensor backward errors.
- Running `module.to(device)` inside ComfyUI's inference-mode node context can produce inference tensors; for controller training, let `ControllerTrainer(..., device=...)` handle device moves inside `inference_mode(False)` and avoid pre-trainer `.to(...)`.
- In host execution contexts where per-step cached log-probs can become detached, recomputing trajectory policy log-probs from stored `(sigma, sampled_mask)` actions against the live controller logits restores a reliable REINFORCE gradient path.
- In recompute paths that index gradient-tracked tensors, any externally provided boolean masks (e.g., eligibility masks passed from node context) must be `detach().clone()`'d first; inference-mode bool tensors can otherwise break autograd bookkeeping.
- For robustness across host runtimes, summing policy log-probs with multiplicative eligibility masks (`(log_probs * eligible_float).sum()`) is safer than boolean advanced indexing (`log_probs[eligible].sum()`), which can behave unpredictably when mask provenance crosses inference-mode boundaries.
- Even when a recomputed objective tensor has `requires_grad=True`, combining it into a loss outside `torch.inference_mode(False)` can strip grad tracking; loss construction and backward must run inside an explicit grad-enabled, inference-disabled context.
- The legacy `TaylorAttentionBackend` and `HybridTaylorAttentionBackend` nodes can be cleanly retired without impacting Flux2TTR v2 by removing their node classes plus standalone support modules (`taylor_attention.py`, `hybrid_attention.py`, `taylor_sym_features.py`, `taylor_triton.py`) and associated tests/benchmarks.
- Flux2TTR quality is less brittle on text/reference-heavy prompts when all conditioning tokens are always treated as landmarks and the landmark budget is reserved exclusively for image tokens; VRAM reservation estimates should include both terms.
- For Comet observability, per-layer metrics should be emitted for all tracked layers at each log tick (not just the current updated layer), and a Pareto-style score (`ready_layers * (1 - worst_ready_ema_cosine_dist)`) gives a compact readiness-vs-quality frontier signal.
- ComfyUI run cleanup can fire after each sampled image; Phase-1 TTR Comet runs stay continuous across queued samplings only if runtime cleanup avoids calling `experiment.end()` and reuses a stable `experiment_key`.
- Readiness gates are more stable with hysteresis: enter at `readiness_threshold`, but only exit above `readiness_threshold * 1.2`, which reduces layer-ready oscillation near the boundary.
- With Kendall-weighted training loss, readiness should use `layer_ema_cosine_dist` instead of `layer_ema_loss`; cosine distance remains interpretable and aligned with quality gating.
- Logging per-layer EMA values at each `flush_run_emas()` event gives direct visibility into readiness-gate inputs at run boundaries.
- Controller inference behavior is easiest to verify with one routing log per step that includes extracted sigma, threshold, and the student-routed layer set; per-layer creation logs alone are not sufficient.
- Sigma-aware controller training is stochastic per diffusion step, so pure deterministic thresholding at inference can collapse to a fixed swap set across all sigmas; exposing stochastic per-step policy sampling in inference better matches the trained policy semantics.
- Deepening the HKR phi MLP from 2 to 3 linear layers and setting `hidden = max(head_dim, 2 * feature_dim)` (with split Q/K by default) increases kernel-map capacity at the cost of one extra hidden-by-hidden projection per network.
- Per-step EMA updates are too sensitive to within-image correlation; accumulating losses/cosine distances across a sampling run and flushing one EMA update on sigma run boundaries stabilizes layer readiness across prompts.
- Sigma-boundary detection is a useful run delimiter but not guaranteed on every training path; a periodic flush trigger (every ~20 accumulated loss updates) prevents EMA/readiness from stalling when sigma transitions are absent.
- In replay distillation, uncertainty-weighted multi-task loss (learned `log_var_huber` and `log_var_cosine`) balances Smooth L1 and cosine-alignment objectives automatically, avoiding fixed manual weights while preserving pressure on directional agreement.
- When runtime objects are constructed under host `torch.inference_mode()`, learned scalar parameters (like loss log-variances) must be created or rebuilt in `inference_mode(False)` before optimizer steps to avoid "inplace update to inference tensor" Adam failures.
- Scalar alpha blending can be too rigid for mixed-attention errors; storing alpha in logit-space and modulating per-token alpha by kernel-vs-landmark disagreement improves correction where branches diverge while retaining backward-compatible initialization and checkpoint migration.
- For adaptive-alpha observability, logging `alpha_sigmoid` per layer to Comet (plus global quantiles) makes it easier to detect saturation/collapse trends during long distillation runs.
- Including `log_var_huber`/`log_var_cosine` in periodic distill snapshot logs helps explain shifts in loss scale and readiness progression without opening Comet.
- Cosine alignment in replay distillation and metric tracking should be computed per token/head over the feature axis (`dim=-1`), not on a fully flattened `[B, H*Nq*D]` vector, otherwise large-magnitude/easy regions can mask hard-token directional errors.
- In multi-run ComfyUI execution, restoring only model weights is not enough for stable continuation; persisting/restoring AdamW optimizer states (layer optimizers and loss-weight optimizer) avoids per-run warmup regressions in learned loss-balance parameters.
- With Kendall loss-weighting, initializing `log_var_cosine` lower than `log_var_huber` (e.g., `-1.0` vs `0.0`) gives cosine alignment stronger early influence before adaptive weighting equilibrates, which helps counter its sparser per-token directional gradient signal.
- Inference-mode provenance can leak into saved optimizer/layer tensors; cloning tensors before persistence and rebuilding any detected inference-backed layer/optimizer pair at use time avoids PyTorch AdamW in-place update failures outside inference mode.
- For kernel/landmark fusion, additive blending (`out_kernel + alpha * out_land`) can push branches to cancel each other; residual gating (`out_kernel + alpha * (out_land - out_kernel)`) gives alpha a cleaner interpolation role and is easier to optimize under replay-constrained training.
- `landmark_max=0` is a useful sentinel for "unlimited" image-token landmarks, but internals must preserve it through runtime/layer/checkpoint normalization and treat it as unlimited in memory estimates (not as `1`), otherwise VRAM planning and behavior diverge from UI intent.
- Adaptive landmark blending is safer when disagreement shifts the alpha logit (`alpha_max * sigmoid(logit + gamma * signal)`) rather than scaling post-sigmoid alpha, because it preserves strict convexity and keeps per-token blend weights bounded by construction.
